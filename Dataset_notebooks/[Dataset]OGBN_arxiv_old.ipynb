{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAT models for OGBN_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, optimizer, data, epochs):\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        acc = (out[data.train_mask].argmax(dim=1) == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, loss, acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    x, edge_index, y = data.x, data.edge_index, data.y\n",
    "    out = model(x, edge_index)\n",
    "    loss = criterion(out[data.test_mask], y[data.test_mask])\n",
    "    acc = (out[data.test_mask].argmax(dim=1) == y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OGBN-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "root = osp.join(osp.dirname(osp.realpath('[Dataset]OGBN_arxiv.ipynb')), '..', 'data', 'arxiv')\n",
    "# transform = T.Compose([T.NormalizeFeatures()])\n",
    "dataset = PygNodePropPredDataset('ogbn-arxiv', root)\n",
    "split_idx = dataset.get_idx_split()\n",
    "evaluator = Evaluator(name='ogbn-arxiv')\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "# make train mask from split_idx['train']\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool).to(device)\n",
    "data.train_mask[split_idx['train']] = True\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool).to(device)\n",
    "data.test_mask[split_idx['test']] = True\n",
    "\n",
    "out_channels = data.y.max().item() + 1\n",
    "data.y = data.y.squeeze_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(data, '/workspace/datasets/OGBN_arxiv.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GAT_Arxiv_2L1H, Loss: 1.3877, Train Accuracy: 0.6029, Test Loss: 1.6097, Test Accuracy: 0.5299\n"
     ]
    }
   ],
   "source": [
    "# Train GAT models on the BA-Shapes dataset\n",
    "from models import GAT_L2_intervention, GAT_L3_intervention\n",
    "\n",
    "out_channels = data.y.max().item() + 1\n",
    "\n",
    "# Define several GAT models with 1, 2, 4, 8 attention heads to be used for 'data.pt', and move them to the GPU device (if available)\n",
    "model1_L2 = GAT_L2_intervention(in_channels=data.num_node_features, hidden_channels=64, out_channels=out_channels, heads=1)\n",
    "# model2_L2 = GAT_L2_intervention(in_channels=data.num_node_features, hidden_channels=64, out_channels=out_channels, heads=2)\n",
    "# model4_L2 = GAT_L2_intervention(in_channels=data.num_node_features, hidden_channels=64, out_channels=out_channels, heads=4)\n",
    "# model8_L2 = GAT_L2_intervention(in_channels=data.num_node_features, hidden_channels=64, out_channels=out_channels, heads=8)\n",
    "# model1_L3 = GAT_L3_intervention(in_channels=data.num_node_features, hidden_channels=64, out_channels=out_channels, heads=1)\n",
    "# model2_L3 = GAT_L3_intervention(in_channels=data.num_node_features, hidden_channels=64, out_channels=out_channels, heads=2)\n",
    "# model4_L3 = GAT_L3_intervention(in_channels=data.num_node_features, hidden_channels=64, out_channels=out_channels, heads=4)\n",
    "# model8_L3 = GAT_L3_intervention(in_channels=data.num_node_features, hidden_channels=64, out_channels=out_channels, heads=8)\n",
    "\n",
    "# Move the models to the GPU device (if available)\n",
    "model1_L2 = model1_L2.to(device)\n",
    "# model2_L2 = model2_L2.to(device)\n",
    "# model4_L2 = model4_L2.to(device)\n",
    "# model8_L2 = model8_L2.to(device)\n",
    "# model1_L3 = model1_L3.to(device)\n",
    "# model2_L3 = model2_L3.to(device)\n",
    "# model4_L3 = model4_L3.to(device)\n",
    "# model8_L3 = model8_L3.to(device)\n",
    "\n",
    "\"\"\"\n",
    "Now we can train all the models and compare their performance.\n",
    "Keep the number of epochs and the learning rate the same for all the models.\n",
    "\"\"\"\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 1600\n",
    "# Define the learning rate\n",
    "lr = 0.01\n",
    "# Prepare the optimizer\n",
    "optimizer1_L2 = torch.optim.Adam(model1_L2.parameters(), lr=lr, weight_decay=0)\n",
    "# optimizer2_L2 = torch.optim.Adam(model2_L2.parameters(), lr=lr, weight_decay=0)\n",
    "# optimizer4_L2 = torch.optim.Adam(model4_L2.parameters(), lr=lr, weight_decay=0)\n",
    "# optimizer8_L2 = torch.optim.Adam(model8_L2.parameters(), lr=lr, weight_decay=0)\n",
    "# optimizer1_L3 = torch.optim.Adam(model1_L3.parameters(), lr=lr, weight_decay=0)\n",
    "# optimizer2_L3 = torch.optim.Adam(model2_L3.parameters(), lr=lr, weight_decay=0)\n",
    "# optimizer4_L3 = torch.optim.Adam(model4_L3.parameters(), lr=lr, weight_decay=0)\n",
    "# optimizer8_L3 = torch.optim.Adam(model8_L3.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "# Train the models\n",
    "model1_L2, loss1_L2, acc1_L2 = train(model=model1_L2, data=data, optimizer=optimizer1_L2, epochs=epochs)\n",
    "# model2_L2, loss2_L2, acc2_L2 = train(model=model2_L2, data=data, optimizer=optimizer2_L2, epochs=epochs)\n",
    "# model4_L2, loss4_L2, acc4_L2 = train(model=model4_L2, data=data, optimizer=optimizer4_L2, epochs=epochs)\n",
    "# model8_L2, loss8_L2, acc8_L2 = train(model=model8_L2, data=data, optimizer=optimizer8_L2, epochs=epochs)\n",
    "# model1_L3, loss1_L3, acc1_L3 = train(model=model1_L3, data=data, optimizer=optimizer1_L3, epochs=epochs)\n",
    "# model2_L3, loss2_L3, acc2_L3 = train(model=model2_L3, data=data, optimizer=optimizer2_L3, epochs=epochs)\n",
    "# model4_L3, loss4_L3, acc4_L3 = train(model=model4_L3, data=data, optimizer=optimizer4_L3, epochs=epochs)\n",
    "# model8_L3, loss8_L3, acc8_L3 = train(model=model8_L3, data=data, optimizer=optimizer8_L3, epochs=epochs)\n",
    "\n",
    "# Test the models\n",
    "test_loss1_L2, test_acc1_L2 = test(model=model1_L2, data=data)\n",
    "# test_loss2_L2, test_acc2_L2 = test(model=model2_L2, data=data)\n",
    "# test_loss4_L2, test_acc4_L2 = test(model=model4_L2, data=data)\n",
    "# test_loss8_L2, test_acc8_L2 = test(model=model8_L2, data=data)\n",
    "# test_loss1_L3, test_acc1_L3 = test(model=model1_L3, data=data)\n",
    "# test_loss2_L3, test_acc2_L3 = test(model=model2_L3, data=data)\n",
    "# test_loss4_L3, test_acc4_L3 = test(model=model4_L3, data=data)\n",
    "# test_loss8_L3, test_acc8_L3 = test(model=model8_L3, data=data)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Model: GAT_Arxiv_2L1H, Loss: {loss1_L2:.4f}, Train Accuracy: {acc1_L2:.4f}, Test Loss: {test_loss1_L2:.4f}, Test Accuracy: {test_acc1_L2:.4f}\")\n",
    "# print(f\"Model: GAT_Arxiv_2L2H, Loss: {loss2_L2:.4f}, Train Accuracy: {acc2_L2:.4f}, Test Loss: {test_loss2_L2:.4f}, Test Accuracy: {test_acc2_L2:.4f}\")\n",
    "# print(f\"Model: GAT_Arxiv_2L4H, Loss: {loss4_L2:.4f}, Train Accuracy: {acc4_L2:.4f}, Test Loss: {test_loss4_L2:.4f}, Test Accuracy: {test_acc4_L2:.4f}\")\n",
    "# print(f\"Model: GAT_Arxiv_2L8H, Loss: {loss8_L2:.4f}, Train Accuracy: {acc8_L2:.4f}, Test Loss: {test_loss8_L2:.4f}, Test Accuracy: {test_acc8_L2:.4f}\")\n",
    "# print(f\"Model: GAT_Arxiv_3L1H, Loss: {loss1_L3:.4f}, Train Accuracy: {acc1_L3:.4f}, Test Loss: {test_loss1_L3:.4f}, Test Accuracy: {test_acc1_L3:.4f}\")\n",
    "# print(f\"Model: GAT_Arxiv_3L2H, Loss: {loss2_L3:.4f}, Train Accuracy: {acc2_L3:.4f}, Test Loss: {test_loss2_L3:.4f}, Test Accuracy: {test_acc2_L3:.4f}\")\n",
    "# print(f\"Model: GAT_Arxiv_3L4H, Loss: {loss4_L3:.4f}, Train Accuracy: {acc4_L3:.4f}, Test Loss: {test_loss4_L3:.4f}, Test Accuracy: {test_acc4_L3:.4f}\")\n",
    "# print(f\"Model: GAT_Arxiv_3L8H, Loss: {loss8_L3:.4f}, Train Accuracy: {acc8_L3:.4f}, Test Loss: {test_loss8_L3:.4f}, Test Accuracy: {test_acc8_L3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model locally\n",
    "torch.save(model1_L2, '/workspace/models/GAT_Arxiv_2L1H.pt')\n",
    "# torch.save(model2_L2, '/workspace/models/GAT_Arxiv_2L2H.pt')\n",
    "# torch.save(model4_L2, '/workspace/models/GAT_Arxiv_2L4H.pt')\n",
    "# torch.save(model8_L2, '/workspace/models/GAT_Arxiv_2L8H.pt')\n",
    "\n",
    "# torch.save(model1_L3, '/workspace/models/GAT_Arxiv_3L1H.pt')\n",
    "# torch.save(model2_L3, '/workspace/models/GAT_Arxiv_3L2H.pt')\n",
    "# torch.save(model4_L3, '/workspace/models/GAT_Arxiv_3L4H.pt')\n",
    "# torch.save(model8_L3, '/workspace/models/GAT_Arxiv_3L8H.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
